{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/selvanathans/ctotalk/blob/main/Llama_4bit_Before_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================================\n",
        "üìã Emotion Classification Baseline Test (BEFORE Fine-Tuning)\n",
        "================================================================================\n",
        "\n",
        "üìã PURPOSE:\n",
        "This notebook tests Llama 3.2 3B's ability to classify emotions BEFORE any\n",
        "fine-tuning. This establishes a baseline to measure improvement after training.\n",
        "\n",
        "üéØ KEY CONCEPT:\n",
        "We're testing what the model ALREADY KNOWS without any emotion training data.\n",
        "This shows the \"before\" picture - after fine-tuning, we'll compare the \"after\".\n",
        "\n",
        "üéØ LEARNING OBJECTIVES:\n",
        "- Load larger models (3B parameters) with 4-bit quantization\n",
        "- Understand chat templates for proper prompt formatting\n",
        "- Test model capabilities on classification tasks\n",
        "- Establish baseline performance metrics\n",
        "- See why fine-tuning is needed for specialized tasks\n",
        "\n",
        "‚öôÔ∏è REQUIREMENTS:\n",
        "- Google Colab with GPU (T4 recommended, 15GB VRAM)\n",
        "- ~5-10 minutes runtime\n",
        "\n",
        "üî¨ WHAT THIS IS:\n",
        "- Tests untrained model on emotion classification\n",
        "- Establishes \"before\" metrics for comparison\n",
        "\n",
        "================================================================================"
      ],
      "metadata": {
        "id": "mXeOYm3dhzz0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tNtEfXOt5U8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# üîß STEP 1: INSTALLATION\n",
        "#============================================================================\n",
        "# Install necessary libraries for running the language model\n",
        "\n",
        "# Optional: Install 'uv' first if you want faster package installation\n",
        "# Uncomment the line below if uv is not already installed\n",
        "# !pip install uv\n",
        "\n",
        "# üí° WHAT IS UV?\n",
        "# 'uv' is a Rust-based Python package installer that's 10-100x faster than pip\n",
        "# It parallelizes downloads and has better dependency resolution\n",
        "# Think of it as \"pip on steroids\" - same commands, much faster execution\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üì¶ Installing Unsloth and Dependencies\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Install Unsloth - The core library for efficient LLM inference\n",
        "!uv pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# üí° WHY UNSLOTH?\n",
        "# - 2-5x faster inference than standard Hugging Face\n",
        "# - 80% less memory usage with same model quality\n",
        "# - Optimized CUDA kernels for attention operations\n",
        "# - Automatic mixed precision handling\n",
        "# - Works seamlessly with Hugging Face ecosystem\n",
        "\n",
        "# Install Transformers - Hugging Face's core library for language models\n",
        "!uv pip install --no-deps transformers>=4.39.0\n",
        "\n",
        "# üí° WHY --no-deps?\n",
        "# Prevents conflicting dependency versions since Unsloth already installed compatible ones\n",
        "# This avoids the common \"dependency hell\" problem in ML projects\n",
        "\n",
        "# Install supporting libraries\n",
        "!uv pip install accelerate bitsandbytes\n",
        "\n",
        "# üí° LIBRARY BREAKDOWN (minimal for baseline testing):\n",
        "# - accelerate: Mixed precision, device management\n",
        "# - bitsandbytes: 4-bit/8-bit quantization for memory efficiency\n",
        "\n",
        "print(\"‚úÖ Installation complete!\\n\")"
      ],
      "metadata": {
        "id": "lQsAJJ5VhaNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# üì¶ STEP 2: IMPORT LIBRARIES & MEMORY CLEANUP\n",
        "#============================================================================\n",
        "\n",
        "import gc            # Garbage collector - Python's automatic memory management\n",
        "import torch         # PyTorch - The deep learning framework powering everything\n",
        "import warnings      # For suppressing non-critical warning messages\n",
        "\n",
        "# üí° WHAT IS PyTorch (torch)?\n",
        "# PyTorch is the foundation for modern deep learning:\n",
        "# - Tensor operations (like NumPy but on GPU)\n",
        "# - Automatic differentiation (calculates gradients for training)\n",
        "# - Neural network building blocks\n",
        "# - GPU acceleration (100-1000x faster than CPU)\n",
        "# Most LLM libraries (Unsloth, Transformers, etc.) are built on PyTorch\n",
        "\n",
        "# Clean up before starting to ensure maximum available memory\n",
        "warnings.filterwarnings(\"ignore\")  # Hide non-critical warnings for cleaner output\n",
        "torch.cuda.empty_cache()           # Clear GPU memory cache\n",
        "gc.collect()                       # Run Python's garbage collector\n",
        "\n",
        "# üí° WHY MEMORY CLEANUP MATTERS:\n",
        "# Colab notebooks can accumulate memory from previous runs\n",
        "# Clearing cache prevents \"CUDA out of memory\" errors\n",
        "# This is especially important when loading large models (3B parameters!)\n",
        "\n",
        "print(\"‚úÖ Memory cleaned and libraries imported\\n\")\n"
      ],
      "metadata": {
        "id": "ontlFItnhhbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# ü§ñ STEP 3: LOAD PRE-TRAINED MODEL WITH 4-BIT QUANTIZATION\n",
        "#============================================================================\n",
        "\n",
        "from unsloth import FastLanguageModel  # Unsloth's optimized model loader\n",
        "\n",
        "# Model configuration constants\n",
        "MODEL_NAME = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"  # Pre-quantized instruct model\n",
        "MAX_SEQ_LENGTH = 512    # Maximum tokens the model can process at once\n",
        "DTYPE = None            # Auto-detect best precision (FP16 for T4, BF16 for A100)\n",
        "LOAD_IN_4BIT = True     # Use 4-bit quantization to save memory\n",
        "\n",
        "# üí° MODEL CHOICE EXPLAINED:\n",
        "# \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
        "# - Llama 3.2: Latest version of Meta's open-source LLM family\n",
        "# - 3B: 3 billion parameters (good balance of quality and speed)\n",
        "# - Instruct: Fine-tuned to follow instructions (better than base models)\n",
        "# - bnb: bitsandbytes quantization (memory-efficient)\n",
        "# - 4bit: Uses 4-bit precision (75% memory reduction)\n",
        "\n",
        "# üí° PARAMETER COUNT COMPARISON:\n",
        "# TinyLlama: 1.1B parameters  ‚Üí Good for demos, limited capability\n",
        "# Llama 3.2 3B: 3B parameters ‚Üí This file, good quality/speed balance\n",
        "# Llama 3 8B: 8B parameters   ‚Üí Higher quality, needs more VRAM\n",
        "# Llama 3 70B: 70B parameters ‚Üí Best quality, requires multi-GPU\n",
        "\n",
        "# üí° MAX_SEQ_LENGTH = 512:\n",
        "# For emotion classification, sentences are short (typically 10-50 tokens)\n",
        "# 512 tokens is more than enough (vs 2048 for longer documents)\n",
        "# Using smaller context window saves memory\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(f\"üîç Loading Model: {MODEL_NAME}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,         # Which model to download/load\n",
        "    max_seq_length=MAX_SEQ_LENGTH, # Context window size (512 tokens ‚âà 400 words)\n",
        "    dtype=DTYPE,                   # None = auto-detect (FP16 on T4, BF16 on A100)\n",
        "    load_in_4bit=LOAD_IN_4BIT     # Enable 4-bit quantization\n",
        ")\n",
        "\n",
        "# üí° WHAT IS QUANTIZATION?\n",
        "# Normal models store weights in 16-bit floating point (FP16):\n",
        "#   3B parameters √ó 2 bytes = 6 GB memory\n",
        "#\n",
        "# 4-bit quantization stores weights in 4 bits:\n",
        "#   3B parameters √ó 0.5 bytes = 1.5 GB memory\n",
        "#\n",
        "# Result: 75% memory reduction with only 1-2% quality loss!\n",
        "# This is why we can run 3B models on free Colab T4 GPU (15GB VRAM)\n",
        "\n",
        "print(f\"‚úÖ Model loaded: {MODEL_NAME}\")\n",
        "print(f\"‚úÖ Parameters: ~3 Billion\")\n",
        "print(f\"‚úÖ Quantization: 4-bit (saves ~75% memory)\")\n",
        "print(f\"‚úÖ Memory usage: ~1.5-2 GB (vs ~6 GB for FP16)\")\n",
        "print(f\"‚úÖ Context window: {MAX_SEQ_LENGTH} tokens (~400 words)\")\n",
        "print(f\"‚úÖ Status: Ready for baseline testing (NO fine-tuning applied)\")\n",
        "print(\"=\"*80 + \"\\n\")\n"
      ],
      "metadata": {
        "id": "pnG_6Y4Yhm2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# üî§ STEP 4: CONFIGURE THE TOKENIZER\n",
        "#============================================================================\n",
        "# The tokenizer converts text into numbers (tokens) that the model understands\n",
        "\n",
        "# Set padding token to be the same as end-of-sequence token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Set padding to happen on the right side of the text\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# üí° WHAT IS A TOKENIZER?\n",
        "# Language models don't understand text - they understand numbers!\n",
        "# The tokenizer breaks text into pieces (tokens) and converts them to IDs:\n",
        "#\n",
        "# Example: \"I'm feeling happy today!\"\n",
        "#   ‚Üí Tokens: [\"I\", \"'m\", \" feeling\", \" happy\", \" today\", \"!\"]\n",
        "#   ‚Üí Token IDs: [40, 2846, 8430, 6380, 3432, 0]\n",
        "#   ‚Üí Model processes these numbers\n",
        "#   ‚Üí Output numbers converted back to text\n",
        "#\n",
        "# Different models use different tokenizers:\n",
        "# - GPT uses Byte-Pair Encoding (BPE)\n",
        "# - Llama uses SentencePiece (similar to BPE)\n",
        "# - Typical vocab size: 32k-100k tokens\n",
        "\n",
        "# üí° WHY PAD_TOKEN = EOS_TOKEN?\n",
        "# When processing batches of different length sentences:\n",
        "# \"I'm happy\" (2 tokens) and \"I'm feeling very happy today\" (6 tokens)\n",
        "# need to be padded to same length for GPU efficiency:\n",
        "# \"I'm happy [PAD] [PAD] [PAD] [PAD]\"\n",
        "# Using EOS (end-of-sequence) as pad token tells model \"ignore these\"\n",
        "\n",
        "print(\"‚úÖ Tokenizer configured\")\n",
        "print(f\"   Vocabulary size: {len(tokenizer):,} tokens\")\n",
        "print(f\"   Padding token: {tokenizer.pad_token}\")\n",
        "print(f\"   Padding side: {tokenizer.padding_side}\")\n",
        "print(f\"   EOS token: {tokenizer.eos_token}\\n\")\n"
      ],
      "metadata": {
        "id": "KuRa7Yv7hpSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# üîÆ STEP 5: CREATE PREDICTION FUNCTION (BASELINE TESTING)\n",
        "#============================================================================\n",
        "\n",
        "def predict_emotion(text):\n",
        "    \"\"\"\n",
        "    Predict emotion from text using the model's chat template.\n",
        "\n",
        "    This tests the UNTRAINED model's baseline performance.\n",
        "    We expect poor/inconsistent results - that's normal and expected!\n",
        "\n",
        "    Args:\n",
        "        text (str): The sentence to classify (e.g., \"I'm feeling happy\")\n",
        "\n",
        "    Returns:\n",
        "        str: Model's predicted emotion response\n",
        "\n",
        "    Process:\n",
        "        1. Format input with system + user messages\n",
        "        2. Apply chat template (converts to model's expected format)\n",
        "        3. Tokenize (text ‚Üí numbers)\n",
        "        4. Generate (model prediction)\n",
        "        5. Decode (numbers ‚Üí text)\n",
        "        6. Extract assistant's response\n",
        "    \"\"\"\n",
        "    # STEP 1: Create message structure (chat format)\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"Identify the emotion in the following sentence and provide the emotion label.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": text\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # üí° MESSAGES FORMAT:\n",
        "    # This is a list of dicts, similar to OpenAI's API format\n",
        "    # - system: Instructions for the AI\n",
        "    # - user: The actual input to process\n",
        "\n",
        "    # STEP 2: Apply chat template (converts messages to model-specific format)\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,           # Return string, not token IDs yet\n",
        "        add_generation_prompt=True  # Add <|assistant|> marker at end\n",
        "    )\n",
        "\n",
        "    # üí° WHAT IS apply_chat_template()?\n",
        "    # Different models expect different formats:\n",
        "    # - Llama uses <|system|>, <|user|>, <|assistant|>\n",
        "    # - GPT uses different tags\n",
        "    # - Some models use [INST] and [/INST]\n",
        "    # apply_chat_template() handles this automatically!\n",
        "\n",
        "    # STEP 3: Tokenize (convert text to numbers) and move to GPU\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # üí° .to(\"cuda\"):\n",
        "    # Moves data from CPU memory to GPU memory\n",
        "    # Model is on GPU, so inputs must be too\n",
        "    # GPU processing is 100-1000x faster than CPU\n",
        "\n",
        "    # STEP 4: Generate prediction\n",
        "    outputs = model.generate(\n",
        "        **inputs,                 # Unpack input token IDs\n",
        "        max_new_tokens=20,       # Maximum length of response (20 tokens ‚âà 15 words)\n",
        "        temperature=0.1,         # Low randomness for consistent classification\n",
        "        do_sample=True           # Use sampling (vs greedy always-pick-top-word)\n",
        "    )\n",
        "\n",
        "    # üí° GENERATION PARAMETERS EXPLAINED:\n",
        "    #\n",
        "    # max_new_tokens=20:\n",
        "    #   - Limits response length to prevent endless generation\n",
        "    #   - For emotion classification, we only need \"0 (sadness)\" ‚âà 3-5 tokens\n",
        "    #   - 20 is safe upper limit\n",
        "    #\n",
        "    # temperature=0.1:\n",
        "    #   - Controls randomness/creativity of response\n",
        "    #   - 0.0 = Always pick most likely word (deterministic)\n",
        "    #   - 0.1 = Mostly likely words (good for factual tasks)\n",
        "    #   - 0.7 = Balanced randomness (good for conversation)\n",
        "    #   - 1.0+ = More creative/random (good for stories)\n",
        "    #   - For classification, we want CONSISTENT answers ‚Üí use low temp\n",
        "    #\n",
        "    # do_sample=True:\n",
        "    #   - True: Use probability sampling (with temperature)\n",
        "    #   - False: Always pick most likely word (greedy decoding)\n",
        "    #   - Even with low temperature, sampling can help avoid repetition\n",
        "\n",
        "    # STEP 5: Decode (convert numbers back to text)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # üí° skip_special_tokens=True:\n",
        "    # Removes special tokens like <|assistant|>, </s>, [PAD]\n",
        "    # Makes output cleaner and human-readable\n",
        "\n",
        "    # STEP 6: Extract just the assistant's response\n",
        "    # The full response includes the prompt too, we only want the answer\n",
        "    response = response.split(\"assistant\")[-1].strip()\n",
        "\n",
        "    # üí° WHY SPLIT ON \"assistant\"?\n",
        "    # Full generated text might be:\n",
        "    # \"system...user...assistant\\n0 (sadness)\"\n",
        "    # We split on \"assistant\" and take the last part: \"0 (sadness)\"\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "RliUxTBZhrjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============================================================================\n",
        "# üß™ STEP 6: RUN BASELINE TESTS (BEFORE FINE-TUNING)\n",
        "#============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"üß™ BASELINE TESTING: Emotion Classification (UNTRAINED MODEL)\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n‚ö†Ô∏è  CRITICAL: This model has NOT been fine-tuned on emotion data!\")\n",
        "print(\"   We expect POOR, VAGUE, or INCORRECT results.\")\n",
        "print(\"   This is the 'BEFORE' picture - fine-tuning will be the 'AFTER'.\\n\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Test sentences covering different emotions from the emotion dataset\n",
        "test_sentences = [\n",
        "    \"i didnt feel humiliated\",\n",
        "    \"im grabbing a minute to post i feel greedy wrong\",\n",
        "    \"i am ever feeling nostalgic about the fireplace i will know that it is still on the property\",\n",
        "    \"i am feeling grouchy\",\n",
        "    \"ive been taking or milligrams or times recommended amount and ive fallen asleep a lot faster but i also feel like so funny\",\n",
        "    \"i feel as confused about life as a teenager or as jaded as a year old man\"\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Running predictions on {len(test_sentences)} test sentences...\\n\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Storage for results\n",
        "results = []\n",
        "\n",
        "# Test each sentence\n",
        "for i, sentence in enumerate(test_sentences, 1):\n",
        "    print(f\"[Test {i}/{len(test_sentences)}]\")\n",
        "    print(f\"üìù Input: {sentence}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Generate prediction\n",
        "    prediction = predict_emotion(sentence)\n",
        "\n",
        "    print(f\"ü§ñ Baseline Prediction: {prediction}\")\n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "    # Store result\n",
        "    results.append({\n",
        "        \"input\": sentence,\n",
        "        \"baseline_output\": prediction\n",
        "    })"
      ],
      "metadata": {
        "id": "7SmLNBKZhulB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}